{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwqlETyd5U0ka/hnk7d3qS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danikayoung16/MAT421/blob/main/Project_Plan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Plan: Developing AI Agents with LLMs\n",
        "\n",
        "## 1. Introduction to the Problem\n",
        "\n",
        "In recent years, Large Language Models (LLMs) such as OpenAI's GPT-4 and DeepSeek have revolutionized natural language processing (NLP) by enabling machines to understand and generate human-like language. However, their true potential is realized when these models are embedded into intelligent agents that can interact dynamically with users, perform complex reasoning, and solve real-world problems.\n",
        "\n",
        "This project aims to explore the development of an LLM-powered AI agent using Python, focused on enabling the model to reason through and solve numerical or algorithmic problems. The agent will serve as a task-specific assistant that can take a prompt, determine the required steps, perform internal calculations or queries, and present a final solution, all while maintaining transparency in reasoning.\n",
        "\n",
        "The central goal of this project is to bridge the gap between large-scale pretrained language models and their practical deployment in intelligent agents capable of autonomous or semi-autonomous decision-making. The work aligns with the broader objective of studying neural networks and AI agents within the scope of MAT 421.\n",
        "\n",
        "## 2. Related Work\n",
        "\n",
        "The field of LLM-based agents has seen a surge in development tools and research interest. Frameworks such as LangChain, AutoGPT, and BabyAGI have demonstrated the viability of using LLMs as the core reasoning engine for multi-step decision-making tasks. These agents combine LLMs with external tools such as calculators, file systems, databases, and web access to solve more complex problems than a single prompt can handle.\n",
        "\n",
        "In an educational context, projects like `SimpleMathSolverVerification.ipynb` and `mathSolverMultVerification.ipynb` showcase how LLMs can be guided to solve mathematical problems and verify answers using structured prompting and validation strategies. These examples form the foundation for designing LLM agents that are not only capable of providing correct answers, but also explaining their steps and flagging inconsistencies in user-provided solutions.\n",
        "\n",
        "Additionally, the `tradingAgent.ipynb` demonstrates how an LLM can be tasked with analyzing financial data, applying rules or strategies, and making trading decisions with human-readable justifications. While trading involves more uncertainty and real-time data, the design philosophy of agent-like behavior remains the same: perception, reasoning, and action.\n",
        "\n",
        "This project draws inspiration from these prior works and seeks to extend them by developing an agent focused on structured reasoning in math and programming tasks using cost-effective APIs like DeepSeek or OpenAI.\n",
        "\n",
        "## 3. Proposed Methodology / Models\n",
        "\n",
        "The proposed AI agent will be designed as a modular Python application that can accept a user prompt and complete the following workflow:\n",
        "\n",
        "1. **Intent Detection**: Determine the task from user input—math problem, code debug, verification request, etc.\n",
        "2. **Structured Prompting**: Generate a tailored prompt template based on the task type.\n",
        "3. **LLM Interaction**: Use OpenAI or DeepSeek APIs to query the LLM with structured prompts.\n",
        "4. **Response Parsing**: Interpret the model's output, extract steps and answers, and format them for user readability.\n",
        "5. **Verification Module** *(for math)*: Perform internal calculations using symbolic math tools (e.g., SymPy) to verify the LLM's solution.\n",
        "6. **User Feedback Loop**: Allow the user to query further, correct misunderstandings, or refine the problem.\n",
        "\n",
        "### Example Agent Modes:\n",
        "\n",
        "- **Math Solver Agent**: Given a numerical or algebraic problem, solve and show steps. Verify correctness and re-prompt if inconsistency is found.\n",
        "- **Code Debug Agent**: Given a buggy code snippet, explain the error and suggest corrections.\n",
        "- **Interactive Tutor Agent** *(optional extension)*: Walk users through solving a math problem step-by-step, pausing for user responses.\n",
        "\n",
        "The LLM will be accessed via the `openai` or `deepseek` Python SDK. For cost-efficiency, DeepSeek is preferred unless OpenAI is needed for comparison. Prompt chaining and few-shot examples will be used to enhance performance.\n",
        "\n",
        "## 4. Experiment Setups\n",
        "\n",
        "The project will be tested with structured datasets and synthetic examples:\n",
        "\n",
        "### Math Solver Testing\n",
        "- A collection of 15–20 math problems from numerical methods and calculus topics.\n",
        "- Evaluation Criteria: Correctness of answers, clarity of explanations, verification accuracy.\n",
        "- Tools: SymPy for symbolic verification, Markdown output for readable steps.\n",
        "\n",
        "### Code Debugging Testing\n",
        "- 10–15 Python code snippets with typical errors (syntax, logic, runtime).\n",
        "- Evaluation Criteria: Ability to identify and explain the error, accuracy of fixes, helpfulness of feedback.\n",
        "\n",
        "### Usability Testing (Optional)\n",
        "- Basic user interface via a command-line tool or notebook form input.\n",
        "- User input and system output logs will be recorded for review and analysis.\n",
        "\n",
        "All experiments will be conducted in Jupyter Notebooks (`.ipynb`) to facilitate reproducibility and presentation. If feasible, results will be visualized using Matplotlib or Plotly.\n",
        "\n",
        "## 5. Expected Results\n",
        "\n",
        "This project is expected to yield a functioning prototype of an LLM-powered AI agent capable of solving and verifying mathematical problems or debugging Python code in an interactive, explainable manner.\n",
        "\n",
        "Expected outcomes include:\n",
        "- An agent with a clear architecture (task identification, prompt generation, model query, post-processing).\n",
        "- Demonstrated improvement over naive single-prompt usage through structured reasoning.\n",
        "- Insight into the cost-performance trade-offs between DeepSeek and OpenAI APIs.\n",
        "- A notebook-based interface that allows for iterative testing and usage.\n",
        "\n",
        "The project also aims to uncover limitations in LLM reasoning, particularly in math verification, and explore potential remedies using symbolic tools or prompt design. This could inform future work in hybrid AI systems that combine statistical models with formal logic or computation engines.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Honors Contract Extension: User Interface and Visual Enhancements\n",
        "\n",
        "As part of the honors enrichment component, this project will include the development of a polished end-user interface with interactive visualizations to enhance usability and engagement.\n",
        "\n",
        "### Web and Mobile Interfaces\n",
        "\n",
        "- Web App: A front-end interface will be developed using Flask or FastAPI, enabling users to interact with the AI agent through a browser. This will include input fields for math problems or code snippets, buttons to trigger agent responses, and sections for output display.\n",
        "\n",
        "\n",
        "### Interactive Visualizations\n",
        "\n",
        "- The agent will include visual representations of problem-solving steps, particularly for math-related queries. This will involve:\n",
        "  - Graphs of functions or data trends.\n",
        "  - Step-by-step equation breakdowns with dynamic rendering (e.g., using MathJax).\n",
        "  - Debugging flowcharts for visualizing code corrections.\n",
        "\n",
        "These enhancements will not only improve the clarity of the agent’s responses but also make the interface more appealing for novice users, educators, and students.\n",
        "\n",
        "---\n",
        "\n",
        "Student Name: Dania Young   \n",
        "Course: MAT 421 – Neural Networks and AI Agents Powered by LLMs  \n"
      ],
      "metadata": {
        "id": "We7tjGZa7czp"
      }
    }
  ]
}